{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff0e86a",
   "metadata": {},
   "source": [
    "## Loads in cleaned spike data\n",
    "- stimuli: color_exchange, luminance_flash, drifting_gratings, and chromatic_gratings\n",
    "\n",
    "@emilyekstrum\n",
    "<br> 11/17/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdadd70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\denmanlab\\.conda\\envs\\topology\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\denmanlab\\.conda\\envs\\topology\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cebra\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "from cebra import CEBRA\n",
    "from cebra.data.helper import OrthogonalProcrustesAlignment\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from dlab.psth_and_raster import trial_by_trial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "plt.style.use(['default', 'seaborn-v0_8-paper'])\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62f2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_dir = r'Z:\\color_representation\\units'\n",
    "stim_dir  = r'Z:\\color_representation\\stim'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c4208",
   "metadata": {},
   "source": [
    "## LGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaafb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse d4\n",
      "Loading Data....\n",
      "torch.Size([34000, 44])\n",
      "Mouse d5\n",
      "Loading Data....\n",
      "torch.Size([34000, 90])\n",
      "Mouse d6\n",
      "Loading Data....\n",
      "torch.Size([34000, 90])\n",
      "Mouse C153\n",
      "Loading Data....\n",
      "torch.Size([34000, 150])\n",
      "Mouse C155\n",
      "Loading Data....\n",
      "torch.Size([34000, 15])\n",
      "Mouse C159\n",
      "Loading Data....\n",
      "torch.Size([34000, 150])\n",
      "Mouse C160\n",
      "Loading Data....\n",
      "torch.Size([34000, 55])\n",
      "Mouse C161\n",
      "Loading Data....\n",
      "torch.Size([34000, 91])\n"
     ]
    }
   ],
   "source": [
    "# color exchange data\n",
    "recordings = ['d4','d5','d6','C153','C155','C159','C160','C161']\n",
    "# recordings = ['C153','C155','C159','C160','C161']\n",
    "\n",
    "stim       = 'color_exchange'\n",
    "#stim      = 'luminance_flash'\n",
    "#stim      = 'drifting_gratings'\n",
    "\n",
    "bin_     = 0.010\n",
    "pre      = 0.100\n",
    "post     = 0.400\n",
    "window   = pre+post\n",
    "n_bins   = int(window/bin_)\n",
    "\n",
    "datas      = []\n",
    "labels0    = []\n",
    "labels1    = []\n",
    "all_probes = []\n",
    "all_ids    = []\n",
    "\n",
    "for m,mouse in enumerate(recordings):\n",
    "    print(f'Mouse {mouse}')\n",
    "\n",
    "    #Load in stimulus dataframe\n",
    "    stim_df  = pd.read_json(glob(os.path.join(stim_dir,f'{mouse}*updated*'))[0])\n",
    "    \n",
    "    #Check for stimulus before loading in more data\n",
    "    if stim not in stim_df.stimulus.unique():\n",
    "        print(f'No {stim}')\n",
    "        continue\n",
    "\n",
    "    stim_df = stim_df.loc[stim_df.stimulus == stim]\n",
    "    \n",
    "    print('Loading Data....')\n",
    "    #Load in unit data\n",
    "    units_df    = pd.read_json(glob(os.path.join(units_dir,f'{mouse}*'))[0])\n",
    "    units_good  = units_df.loc[(units_df.qmLabel == 'GOOD')|(units_df.qmLabel == 'NON-SOMA GOOD')]\n",
    "    # units_good  = units_good.loc[(units_good.region.str.contains('Primary visual'))|(units_good.region.str.contains('lateral geniculate'))]\n",
    "    units_good  = units_good.loc[(units_good.region.str.contains('lateral geniculate'))]\n",
    "\n",
    "    units_good.reset_index(inplace=True,drop=True)\n",
    "    units_df    = None\n",
    "\n",
    "    if len(units_good) < 1:\n",
    "        continue\n",
    "\n",
    "    #Wrangle stimulus-relevant data\n",
    "    stim_df     = stim_df.loc[(stim_df.stimulus == stim)]\n",
    "    stim_df.reset_index(drop=True,inplace=True)\n",
    "    stim_times  = stim_df.loc[:,'start_time'].values\n",
    "    stim_length = stim_times[-1]-stim_times[0]\n",
    "    \n",
    "    if len(stim_df) > 680:\n",
    "        trial_idx  = random.sample(range(len(stim_df)-1),680)\n",
    "        trial_idx.sort()\n",
    "        stim_times = stim_times[1:][trial_idx]\n",
    "\n",
    "        M = np.repeat(stim_df.green.values.astype(float)[1:][trial_idx],n_bins) # M opsin\n",
    "        S = np.repeat(stim_df.uv.values.astype(float)[1:][trial_idx],n_bins) # S opsin\n",
    "        L = (M+S)/2 # luminance - additive opsin signal\n",
    "        C = (M-S)/2 # contrast - subtractive opsin signal\n",
    "\n",
    "        dM = np.repeat(np.diff(stim_df.green.values.astype(float))[trial_idx],n_bins)/2\n",
    "        dS = np.repeat(np.diff(stim_df.uv.values.astype(float))[trial_idx],n_bins)/2\n",
    "        dL = (M+S)/2\n",
    "        dC = (M-S)/2\n",
    "    else:\n",
    "        M = np.repeat(stim_df.green.values.astype(float),n_bins)\n",
    "        S = np.repeat(stim_df.uv.values.astype(float),n_bins)\n",
    "        L = (M+S)/2\n",
    "        C = (M-S)/2\n",
    "\n",
    "        dM = np.repeat(np.diff(np.insert(stim_df.green.values.astype(float),0,0)),n_bins)/2\n",
    "        dS = np.repeat(np.diff(np.insert(stim_df.uv.values.astype(float),0,0)),n_bins)/2\n",
    "        dL = (dM+dS)/2\n",
    "        dC = (dM-dS)/2\n",
    "\n",
    "    labels0.append(torch.from_numpy(np.array([M,S,L,C]).T))\n",
    "    labels1.append(torch.from_numpy(np.array([dM,dS,dL,dC]).T))\n",
    "\n",
    "    #Organize spike data (y)\n",
    "    n_trials = len(stim_times)\n",
    "    n_units  = len(units_good)\n",
    "\n",
    "    probes   = []\n",
    "    cIDs     = []\n",
    "    rec_data = []\n",
    "\n",
    "    k = 0\n",
    "    for i,row in units_good.iterrows():\n",
    "        spike_times = np.array(row.times)\n",
    "        stim_spikes = spike_times[(spike_times > stim_times[0]) & (spike_times < stim_times[-1]+post)]\n",
    "        \n",
    "        if len(stim_spikes)/stim_length < 0.5:\n",
    "            continue\n",
    "        \n",
    "        psth,var,edges,bytrial = trial_by_trial(stim_spikes, stim_times, pre, post, bin_)\n",
    "        \n",
    "        rec_data.append(bytrial.ravel())\n",
    "        \n",
    "        probes.append(row.probe)\n",
    "        cIDs.append(row.cluster_id)\n",
    "        k +=1\n",
    "    datas.append(torch.from_numpy(np.array(rec_data).T))\n",
    "    all_probes.append(np.array(probes))\n",
    "    all_ids.append(np.array(cIDs))\n",
    "\n",
    "    # dM = np.repeat(np.diff(stim_df.green.values.astype(float)),n_bins)\n",
    "    # dS = np.repeat(np.diff(stim_df.uv.values.astype(float)),n_bins)\n",
    "    # dL = (dM+dS)/2\n",
    "    # dC = (dM-dS)/2\n",
    "\n",
    "\n",
    "    print(torch.from_numpy(np.array(rec_data).T).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f8863",
   "metadata": {},
   "source": [
    "## Load in other stimulus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stim       = 'chromatic_gratings'\n",
    "recordings = ['d4','d5','d6','C153','C155','C159','C160','C161']\n",
    "\n",
    "units_dir = r'Z:\\color_representation\\units'\n",
    "stim_dir  = r'Z:\\color_representation\\stim'\n",
    "res_dir   = 'G:\\\\'  # not used\n",
    "\n",
    "bin_     = 0.010\n",
    "pre      = 0.500\n",
    "post     = 1.500\n",
    "window   = pre + post\n",
    "n_bins   = int(window / bin_)\n",
    "\n",
    "datas      = []  # list[torch.Tensor], each (n_trials*n_bins, n_units)\n",
    "all_probes = []  # list[np.ndarray] per recording\n",
    "all_ids    = []  # list[np.ndarray] per recording\n",
    "\n",
    "for m, mouse in enumerate(recordings):\n",
    "    print(f'Mouse {mouse}')\n",
    "\n",
    "    # Load stimulus dataframe (keep original file pattern)\n",
    "    stim_matches = glob(os.path.join(stim_dir, f'{mouse}*'))\n",
    "    if not stim_matches:\n",
    "        print('No stim file found')\n",
    "        continue\n",
    "    stim_df = pd.read_json(stim_matches[0])\n",
    "\n",
    "    # Ensure target stimulus present\n",
    "    if stim not in stim_df.stimulus.unique():\n",
    "        print(f'No {stim}')\n",
    "        continue\n",
    "\n",
    "    # Keep only this stimulus & reset index\n",
    "    stim_df = stim_df.loc[stim_df.stimulus == stim].reset_index(drop=True)\n",
    "\n",
    "    print('Loading Data....')\n",
    "    # Load units\n",
    "    unit_matches = glob(os.path.join(units_dir, f'{mouse}*'))\n",
    "    if not unit_matches:\n",
    "        print('No units file found')\n",
    "        continue\n",
    "    units_df = pd.read_json(unit_matches[0])\n",
    "\n",
    "    # Good units filter (Primary visual OR LGN, same as your original)\n",
    "    units_good = units_df.loc[\n",
    "        (units_df.qmLabel.isin(['GOOD', 'NON-SOMA GOOD'])) &\n",
    "        (\n",
    "            units_df.region.str.contains('Primary visual', case=False, na=False) |\n",
    "            units_df.region.str.contains('lateral geniculate', case=False, na=False)\n",
    "        )\n",
    "    ].reset_index(drop=True)\n",
    "    units_df = None\n",
    "\n",
    "    if len(units_good) < 1:\n",
    "        print('No good units after filtering')\n",
    "        continue\n",
    "\n",
    "    # Stim timing\n",
    "    stim_times_full = stim_df.loc[:, 'start_time'].values\n",
    "\n",
    "\n",
    "    if len(stim_df) > 680:\n",
    "        # sample indices from range(len(stim_df)-1), then shift times by 1 like script #2\n",
    "        trial_idx = random.sample(range(len(stim_df) - 1), 680)\n",
    "        trial_idx.sort()\n",
    "        stim_times = stim_times_full[1:][trial_idx]\n",
    "    else:\n",
    "        stim_times = stim_times_full\n",
    "\n",
    "    # Compute stimulus span for firing-rate heuristic\n",
    "    stim_length = stim_times[-1] - stim_times[0]\n",
    "\n",
    "    # Build datas (flattened bytrial per unit -> columns)\n",
    "    probes   = []\n",
    "    cIDs     = []\n",
    "    rec_data = []\n",
    "\n",
    "    kept_units = 0\n",
    "    for _, row in units_good.iterrows():\n",
    "        spike_times = np.array(row.times)\n",
    "        stim_spikes = spike_times[\n",
    "            (spike_times > stim_times[0]) & (spike_times < stim_times[-1] + post)\n",
    "        ]\n",
    "        \n",
    "        if len(stim_spikes) / stim_length < 0.5:\n",
    "            continue\n",
    "\n",
    "        psth, var, edges, bytrial = trial_by_trial(\n",
    "            stim_spikes, stim_times, pre, post, bin_\n",
    "        )\n",
    "\n",
    "        # Flatten (n_trials * n_bins,) for this unit\n",
    "        rec_data.append(bytrial.ravel())\n",
    "        probes.append(row.probe)\n",
    "        cIDs.append(row.cluster_id)\n",
    "        kept_units += 1\n",
    "\n",
    "    if kept_units == 0:\n",
    "        print('No units passed firing-rate threshold')\n",
    "        continue\n",
    "\n",
    "    # Shape: (n_trials*n_bins, n_units)\n",
    "    data_tensor = torch.from_numpy(np.array(rec_data).T)\n",
    "    datas.append(data_tensor)\n",
    "    all_probes.append(np.array(probes))\n",
    "    all_ids.append(np.array(cIDs))\n",
    "\n",
    "    print(f'{mouse}: {data_tensor.shape} (timeÃ—units)')\n",
    "\n",
    "# datas      -> list[torch.Tensor], each (n_trials*n_bins, n_units)\n",
    "# all_probes -> list[np.ndarray]\n",
    "# all_ids    -> list[np.ndarray]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b1d436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse d4\n",
      "Loading Data....\n",
      "torch.Size([34000, 44])\n",
      "Mouse d5\n",
      "Loading Data....\n",
      "torch.Size([34000, 90])\n",
      "Mouse d6\n",
      "Loading Data....\n",
      "torch.Size([34000, 90])\n",
      "Mouse C153\n",
      "Loading Data....\n",
      "torch.Size([34000, 150])\n",
      "Mouse C155\n",
      "Loading Data....\n",
      "torch.Size([34000, 15])\n",
      "Mouse C159\n",
      "Loading Data....\n",
      "torch.Size([34000, 150])\n",
      "Mouse C160\n",
      "Loading Data....\n",
      "torch.Size([34000, 55])\n",
      "Mouse C161\n",
      "Loading Data....\n",
      "torch.Size([34000, 91])\n"
     ]
    }
   ],
   "source": [
    "#color exchange data\n",
    "units_dir = r'Z:\\color_representation\\units'\n",
    "stim_dir  = r'Z:\\color_representation\\stim'\n",
    "\n",
    "recordings = ['d4','d5','d6','C153','C155','C159','C160','C161']\n",
    "# recordings = ['C153','C155','C159','C160','C161']\n",
    "\n",
    "stim       = 'color_exchange'\n",
    "#stim      = 'chromatic_gratings'\n",
    "#stim      = 'luminance_flash'\n",
    "#stim      = 'drifting_gratings'\n",
    "\n",
    "bin_     = 0.010\n",
    "pre      = 0.100\n",
    "post     = 0.400\n",
    "window   = pre+post\n",
    "n_bins   = int(window/bin_)\n",
    "\n",
    "datas      = []\n",
    "labels0    = []\n",
    "labels1    = []\n",
    "all_probes = []\n",
    "all_ids    = []\n",
    "\n",
    "for m,mouse in enumerate(recordings):\n",
    "    print(f'Mouse {mouse}')\n",
    "\n",
    "    #Load in stimulus dataframe\n",
    "    stim_df  = pd.read_json(glob(os.path.join(stim_dir,f'{mouse}*updated*'))[0])\n",
    "    \n",
    "    #Check for stimulus before loading in more data\n",
    "    if stim not in stim_df.stimulus.unique():\n",
    "        print(f'No {stim}')\n",
    "        continue\n",
    "\n",
    "    stim_df = stim_df.loc[stim_df.stimulus == stim]\n",
    "    \n",
    "    print('Loading Data....')\n",
    "    #Load in unit data\n",
    "    units_df    = pd.read_json(glob(os.path.join(units_dir,f'{mouse}*'))[0])\n",
    "    units_good  = units_df.loc[(units_df.qmLabel == 'GOOD')|(units_df.qmLabel == 'NON-SOMA GOOD')]\n",
    "    # units_good  = units_good.loc[(units_good.region.str.contains('Primary visual'))|(units_good.region.str.contains('lateral geniculate'))]\n",
    "    units_good  = units_good.loc[(units_good.region.str.contains('lateral geniculate'))]\n",
    "\n",
    "    units_good.reset_index(inplace=True,drop=True)\n",
    "    units_df    = None\n",
    "\n",
    "    if len(units_good) < 1:\n",
    "        continue\n",
    "\n",
    "    #Wrangle stimulus-relevant data\n",
    "    stim_df     = stim_df.loc[(stim_df.stimulus == stim)]\n",
    "    stim_df.reset_index(drop=True,inplace=True)\n",
    "    stim_times  = stim_df.loc[:,'start_time'].values\n",
    "    stim_length = stim_times[-1]-stim_times[0]\n",
    "    \n",
    "    if len(stim_df) > 680:\n",
    "        trial_idx  = random.sample(range(len(stim_df)-1),680)\n",
    "        trial_idx.sort()\n",
    "        stim_times = stim_times[1:][trial_idx]\n",
    "\n",
    "        M = np.repeat(stim_df.green.values.astype(float)[1:][trial_idx],n_bins)\n",
    "        S = np.repeat(stim_df.uv.values.astype(float)[1:][trial_idx],n_bins)\n",
    "        L = (M+S)/2\n",
    "        C = (M-S)/2\n",
    "\n",
    "        dM = np.repeat(np.diff(stim_df.green.values.astype(float))[trial_idx],n_bins)/2\n",
    "        dS = np.repeat(np.diff(stim_df.uv.values.astype(float))[trial_idx],n_bins)/2\n",
    "        dL = (M+S)/2\n",
    "        dC = (M-S)/2\n",
    "    else:\n",
    "        M = np.repeat(stim_df.green.values.astype(float),n_bins)\n",
    "        S = np.repeat(stim_df.uv.values.astype(float),n_bins)\n",
    "        L = (M+S)/2\n",
    "        C = (M-S)/2\n",
    "\n",
    "        dM = np.repeat(np.diff(np.insert(stim_df.green.values.astype(float),0,0)),n_bins)/2\n",
    "        dS = np.repeat(np.diff(np.insert(stim_df.uv.values.astype(float),0,0)),n_bins)/2\n",
    "        dL = (dM+dS)/2\n",
    "        dC = (dM-dS)/2\n",
    "\n",
    "    labels0.append(torch.from_numpy(np.array([M,S,L,C]).T))\n",
    "    labels1.append(torch.from_numpy(np.array([dM,dS,dL,dC]).T))\n",
    "\n",
    "    #Organize spike data (y)\n",
    "    n_trials = len(stim_times)\n",
    "    n_units  = len(units_good)\n",
    "\n",
    "    probes   = []\n",
    "    cIDs     = []\n",
    "    rec_data = []\n",
    "\n",
    "    k = 0\n",
    "    for i,row in units_good.iterrows():\n",
    "        spike_times = np.array(row.times)\n",
    "        stim_spikes = spike_times[(spike_times > stim_times[0]) & (spike_times < stim_times[-1]+post)]\n",
    "        \n",
    "        if len(stim_spikes)/stim_length < 0.5:\n",
    "            continue\n",
    "        \n",
    "        psth,var,edges,bytrial = trial_by_trial(stim_spikes, stim_times, pre, post, bin_)\n",
    "        \n",
    "        rec_data.append(bytrial.ravel())\n",
    "        \n",
    "        probes.append(row.probe)\n",
    "        cIDs.append(row.cluster_id)\n",
    "        k +=1\n",
    "    datas.append(torch.from_numpy(np.array(rec_data).T))\n",
    "    all_probes.append(np.array(probes))\n",
    "    all_ids.append(np.array(cIDs))\n",
    "\n",
    "    # dM = np.repeat(np.diff(stim_df.green.values.astype(float)),n_bins)\n",
    "    # dS = np.repeat(np.diff(stim_df.uv.values.astype(float)),n_bins)\n",
    "    # dL = (dM+dS)/2\n",
    "    # dC = (dM-dS)/2\n",
    "\n",
    "\n",
    "    print(torch.from_numpy(np.array(rec_data).T).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save datas and recordings \n",
    "to_save = {\n",
    "    'datas': datas,\n",
    "    'recordings': recordings\n",
    "}\n",
    "\n",
    "with open('C:/Users/denmanlab/Desktop/Emily_rotation/data_to_run_at_home/LGNcolor_exchange.pkl', 'wb') as f:\n",
    "    pkl.dump(to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a92c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse C155\n",
      "Loading Data....\n",
      "C155: torch.Size([136000, 15])\n",
      "Mouse C159\n",
      "Loading Data....\n",
      "C159: torch.Size([136000, 177])\n",
      "Mouse C161\n",
      "Loading Data....\n",
      "C161: torch.Size([136000, 95])\n"
     ]
    }
   ],
   "source": [
    "# chromatic gratings data\n",
    "stim       = 'chromatic_gratings'\n",
    "recordings = ['C155','C159','C161']\n",
    "\n",
    "units_dir = r'Z:\\color_representation\\units'\n",
    "stim_dir  = r'Z:\\color_representation\\stim'\n",
    "res_dir   = 'G:\\\\'  # not used \n",
    "\n",
    "bin_     = 0.010\n",
    "pre      = 0.500\n",
    "post     = 1.500\n",
    "window   = pre + post\n",
    "n_bins   = int(window / bin_)\n",
    "\n",
    "datas      = []  # list[torch.Tensor], each (n_trials*n_bins, n_units)\n",
    "all_probes = []  # list[np.ndarray] per recording\n",
    "all_ids    = []  # list[np.ndarray] per recording\n",
    "\n",
    "for m, mouse in enumerate(recordings):\n",
    "    print(f'Mouse {mouse}')\n",
    "\n",
    "    # Load stimulus dataframe\n",
    "    stim_df = pd.read_json(glob(os.path.join(stim_dir, f'{mouse}*updated*'))[0])\n",
    "\n",
    "    # Ensure target stimulus present\n",
    "    if stim not in stim_df.stimulus.unique():\n",
    "        print(f'No {stim}')\n",
    "        continue\n",
    "\n",
    "    # Keep only this stimulus & reset index\n",
    "    stim_df = stim_df.loc[stim_df.stimulus == stim]\n",
    "\n",
    "    print('Loading Data....')\n",
    "    #Load in unit data\n",
    "    units_df    = pd.read_json(glob(os.path.join(units_dir,f'{mouse}*'))[0])\n",
    "    units_good  = units_df.loc[(units_df.qmLabel == 'GOOD')|(units_df.qmLabel == 'NON-SOMA GOOD')]\n",
    "    # units_good  = units_good.loc[(units_good.region.str.contains('Primary visual'))|(units_good.region.str.contains('lateral geniculate'))]\n",
    "    units_good  = units_good.loc[(units_good.region.str.contains('lateral geniculate'))]\n",
    "\n",
    "    units_good.reset_index(inplace=True,drop=True)\n",
    "    units_df    = None\n",
    "\n",
    "    if len(units_good) < 1:\n",
    "        continue\n",
    "\n",
    "    # Stim timing\n",
    "    stim_times_full = stim_df.loc[:, 'start_time'].values\n",
    "\n",
    "    #Wrangle stimulus-relevant data\n",
    "    stim_df     = stim_df.loc[(stim_df.stimulus == stim)]\n",
    "    stim_df.reset_index(drop=True,inplace=True)\n",
    "    stim_times  = stim_df.loc[:,'start_time'].values\n",
    "    stim_length = stim_times[-1]-stim_times[0]\n",
    "\n",
    "    if len(stim_df) > 680:\n",
    "        trial_idx = random.sample(range(len(stim_df) - 1), 680)\n",
    "        trial_idx.sort()\n",
    "        stim_times = stim_times_full[1:][trial_idx]\n",
    "    else:\n",
    "        stim_times = stim_times_full\n",
    "\n",
    "    # Compute stimulus span for firing-rate heuristic\n",
    "    stim_length = stim_times[-1] - stim_times[0]\n",
    "\n",
    "    # Build datas (flattened bytrial per unit -> columns)\n",
    "    probes   = []\n",
    "    cIDs     = []\n",
    "    rec_data = []\n",
    "\n",
    "    kept_units = 0\n",
    "    for _, row in units_good.iterrows():\n",
    "        spike_times = np.array(row.times)\n",
    "        stim_spikes = spike_times[\n",
    "            (spike_times > stim_times[0]) & (spike_times < stim_times[-1] + post)\n",
    "        ]\n",
    "        \n",
    "        if len(stim_spikes) / stim_length < 0.5:\n",
    "            continue\n",
    "\n",
    "        psth, var, edges, bytrial = trial_by_trial(\n",
    "            stim_spikes, stim_times, pre, post, bin_\n",
    "        )\n",
    "\n",
    "        # Flatten (n_trials * n_bins,) for this unit\n",
    "        rec_data.append(bytrial.ravel())\n",
    "        probes.append(row.probe)\n",
    "        cIDs.append(row.cluster_id)\n",
    "        kept_units += 1\n",
    "\n",
    "    if kept_units == 0:\n",
    "        print('No units passed firing-rate threshold')\n",
    "        continue\n",
    "\n",
    "    # Shape: (n_trials*n_bins, n_units)\n",
    "    data_tensor = torch.from_numpy(np.array(rec_data).T)\n",
    "    datas.append(data_tensor)\n",
    "    all_probes.append(np.array(probes))\n",
    "    all_ids.append(np.array(cIDs))\n",
    "\n",
    "    print(f'{mouse}: {data_tensor.shape}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save datas and recordings \n",
    "to_save = {\n",
    "    'datas': datas,\n",
    "    'recordings': recordings\n",
    "}\n",
    "\n",
    "with open('C:/Users/denmanlab/Desktop/Emily_rotation/data_to_run_at_home/LGNchromatic_gratings.pkl', 'wb') as f:\n",
    "    pkl.dump(to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113e5c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stim files for C161: ['Z:\\\\color_representation\\\\stim\\\\C161_stim.json', 'Z:\\\\color_representation\\\\stim\\\\C161_stim_updated.json']\n",
      "Stimulus types in Z:\\color_representation\\stim\\C161_stim_updated.json: ['luminance_flash' 'spatioluminance_noise' 'spatiochromatic_noise'\n",
      " 'color_exchange' 'drifting_gratings' 'chromatic_gratings' 'sweeping_bar']\n",
      "          stimulus   start_time green   uv  contrast  temporal_frequency  \\\n",
      "0  luminance_flash  3686.609433   256  256        -1                  -1   \n",
      "1  luminance_flash  3689.603167     0    0        -1                  -1   \n",
      "2  luminance_flash  3692.605900   256  256        -1                  -1   \n",
      "3  luminance_flash  3695.608400     0    0        -1                  -1   \n",
      "4  luminance_flash  3698.612733   256  256        -1                  -1   \n",
      "\n",
      "   spatial_frequency  orientation  stimulus_index  \n",
      "0               -1.0           -1              -1  \n",
      "1               -1.0           -1              -1  \n",
      "2               -1.0           -1              -1  \n",
      "3               -1.0           -1              -1  \n",
      "4               -1.0           -1              -1  \n",
      "Stimulus column found with 7 unique values.\n"
     ]
    }
   ],
   "source": [
    "# check stimulus recordings for a mouse\n",
    "mouse = 'C161'\n",
    "stim_dir  = r'Z:\\color_representation\\stim'\n",
    "\n",
    "stim_matches = glob(os.path.join(stim_dir, f'{mouse}*'))\n",
    "print(f\"Stim files for {mouse}: {stim_matches}\")\n",
    "\n",
    "stim_file = stim_matches[1]\n",
    "stim_df = pd.read_json(stim_file)\n",
    "print(f\"Stimulus types in {stim_file}: {stim_df.stimulus.unique()}\")\n",
    "print(stim_df.head())\n",
    "\n",
    "if 'stimulus' in stim_df.columns:\n",
    "    print(f\"Stimulus column found with {stim_df['stimulus'].nunique()} unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0edba577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse d5\n",
      "Loading Data....\n",
      "torch.Size([128000, 92])\n",
      "Mouse C155\n",
      "Loading Data....\n",
      "torch.Size([64000, 16])\n",
      "Mouse C159\n",
      "Loading Data....\n",
      "torch.Size([64000, 160])\n",
      "Mouse C160\n",
      "Loading Data....\n",
      "torch.Size([64000, 51])\n",
      "Mouse C161\n",
      "Loading Data....\n",
      "torch.Size([64000, 94])\n"
     ]
    }
   ],
   "source": [
    "# drifting gratings data\n",
    "stim       = 'drifting_gratings'\n",
    "# recordings = ['d4','d5','d6','C153','C155','C159','C160','C161']\n",
    "recordings = ['d5','C155','C159','C160','C161']\n",
    "\n",
    "units_dir = r'Z:\\color_representation\\units' \n",
    "stim_dir  = r'Z:\\color_representation\\stim'\n",
    "\n",
    "bin_     = 0.010\n",
    "pre      = 0.500\n",
    "post     = 1.500\n",
    "window   = pre+post\n",
    "n_bins   = int(window/bin_)\n",
    "\n",
    "datas      = []\n",
    "all_probes = []\n",
    "all_ids    = []\n",
    "\n",
    "for m,mouse in enumerate(recordings):\n",
    "    print(f'Mouse {mouse}')\n",
    "\n",
    "    #Load in stimulus dataframe\n",
    "    stim_df  = pd.read_json(glob(os.path.join(stim_dir,f'{mouse}*updated*'))[0])\n",
    "    \n",
    "    #Check for stimulus before loading in more data\n",
    "    if stim not in stim_df.stimulus.unique():\n",
    "        print(f'No {stim}')\n",
    "        continue\n",
    "\n",
    "    stim_df = stim_df.loc[stim_df.stimulus == stim]\n",
    "    \n",
    "    print('Loading Data....')\n",
    "    #Load in unit data\n",
    "    units_df    = pd.read_json(glob(os.path.join(units_dir,f'{mouse}*'))[0])\n",
    "    units_good  = units_df.loc[(units_df.qmLabel == 'GOOD')|(units_df.qmLabel == 'NON-SOMA GOOD')]\n",
    "    #units_good  = units_good.loc[(units_good.region.str.contains('Primary visual'))|(units_good.region.str.contains('lateral geniculate'))]\n",
    "    units_good  = units_good.loc[(units_good.region.str.contains('lateral geniculate'))]\n",
    "    units_good.reset_index(inplace=True,drop=True)\n",
    "    units_df    = None\n",
    "\n",
    "    if len(units_good) < 1:\n",
    "        continue        \n",
    "\n",
    "    # Stim timing\n",
    "    stim_times_full = stim_df.loc[:, 'start_time'].values\n",
    "\n",
    "    #Wrangle stimulus-relevant data\n",
    "    stim_df     = stim_df.loc[(stim_df.stimulus == stim)]\n",
    "    stim_times  = stim_df.loc[:,'start_time'].values\n",
    "    stim_length = stim_times[-1]-stim_times[0]\n",
    "    \n",
    "    #Organize spike data (y)\n",
    "    n_trials = len(stim_times)\n",
    "    n_units  = len(units_good)\n",
    "    \n",
    "    yall = np.zeros((n_trials,n_bins,n_units))\n",
    "\n",
    "    probes = []\n",
    "    cIDs   = []\n",
    "    rec_data = []\n",
    "    k = 0\n",
    "    for i,row in units_good.iterrows():\n",
    "        spike_times = np.array(row.times)\n",
    "        stim_spikes = spike_times[(spike_times > stim_times[0]) & (spike_times < stim_times[-1]+post)]\n",
    "        \n",
    "        if len(stim_spikes)/stim_length < 0.5:\n",
    "            continue\n",
    "        \n",
    "        psth,var,edges,bytrial = trial_by_trial(stim_spikes, stim_times, pre, post, bin_)\n",
    "        \n",
    "        rec_data.append(bytrial.ravel())\n",
    "        probes.append(row.probe)\n",
    "        cIDs.append(row.cluster_id)\n",
    "        k +=1\n",
    "    \n",
    "    #Organize stimulus data\n",
    "    orientation       = np.array(stim_df.orientation.values).astype(float)\n",
    "    spatial_frequency = np.array(stim_df.spatial_frequency.values).astype(float)\n",
    "    \n",
    "    conditions = [orientation,spatial_frequency]\n",
    "\n",
    "    Xall = np.ones((n_trials,n_bins,len(conditions)+1))\n",
    "\n",
    "    for s in range(len(stim_times)):\n",
    "        for c,cond in enumerate(conditions):\n",
    "            if c == 1:\n",
    "                Xall[s,:int(0.100/bin_)+1,c] = 0\n",
    "                Xall[s,int(0.100/bin_)+1:,c] = cond[s]\n",
    "            else:\n",
    "                Xall[s,:int(0.100/bin_)+1,c] = -1\n",
    "                Xall[s,int(0.100/bin_)+1:,c] = cond[s]\n",
    "\n",
    "    if k == 0:\n",
    "        print('No units passed firing-rate threshold')\n",
    "        continue\n",
    "\n",
    "    datas.append(torch.from_numpy(np.array(rec_data).T))\n",
    "    all_probes.append(np.array(probes))\n",
    "    all_ids.append(np.array(cIDs))\n",
    "\n",
    "    print(torch.from_numpy(np.array(rec_data).T).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43492433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save datas and recordings \n",
    "to_save = {\n",
    "    'datas': datas,\n",
    "    'recordings': recordings\n",
    "}\n",
    "\n",
    "with open('C:/Users/denmanlab/Desktop/Emily_rotation/data_to_run_at_home/LGNdrifting_gratings.pkl', 'wb') as f:\n",
    "    pkl.dump(to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ed7816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse d4\n",
      "Loading Data....\n",
      "d4: torch.Size([5000, 37])\n",
      "Mouse d5\n",
      "Loading Data....\n",
      "d5: torch.Size([5000, 65])\n",
      "Mouse d6\n",
      "Loading Data....\n",
      "d6: torch.Size([5000, 79])\n",
      "Mouse C155\n",
      "Loading Data....\n",
      "C155: torch.Size([5000, 12])\n",
      "Mouse C159\n",
      "Loading Data....\n",
      "C159: torch.Size([5000, 145])\n",
      "Mouse C160\n",
      "Loading Data....\n",
      "C160: torch.Size([5000, 62])\n",
      "Mouse C161\n",
      "Loading Data....\n",
      "C161: torch.Size([5000, 72])\n"
     ]
    }
   ],
   "source": [
    "# luminance flash data\n",
    "stim       = 'luminance_flash'\n",
    "# recordings = ['d6','C153','C155','C159','C160','C161']\n",
    "recordings = ['d4','d5','d6','C155','C159','C160','C161']\n",
    "# recordings = ['d4','d5',]\n",
    "\n",
    "units_dir = r'Z:\\color_representation\\units'\n",
    "stim_dir  = r'Z:\\color_representation\\stim'\n",
    "res_dir   = 'G:\\\\'\n",
    "\n",
    "bin_     = 0.010\n",
    "window   = 0.500\n",
    "n_bins   = int(window/bin_)\n",
    "\n",
    "datas      = []\n",
    "all_probes = []\n",
    "all_ids    = []\n",
    "\n",
    "for m,mouse in enumerate(recordings):\n",
    "    print(f'Mouse {mouse}')\n",
    "\n",
    "    #Load in stimulus dataframe\n",
    "    stim_df  = pd.read_json(glob(os.path.join(stim_dir,f'{mouse}*updated*'))[0])\n",
    "    \n",
    "    #Check for stimulus before loading in more data\n",
    "    if stim not in stim_df.stimulus.unique():\n",
    "        print(f'No {stim}')\n",
    "        continue\n",
    "    \n",
    "    print('Loading Data....')\n",
    "    #Load in unit data\n",
    "    units_df    = pd.read_json(glob(os.path.join(units_dir,f'{mouse}*'))[0])\n",
    "    units_good  = units_df.loc[(units_df.qmLabel == 'GOOD')|(units_df.qmLabel == 'NON-SOMA GOOD')]\n",
    "    #units_good  = units_good.loc[(units_good.region.str.contains('Primary visual'))|(units_good.region.str.contains('lateral geniculate'))]\n",
    "    units_good  = units_good.loc[(units_good.region.str.contains('lateral geniculate'))]\n",
    "    units_good.reset_index(inplace=True,drop=True)\n",
    "    units_df    = None\n",
    "\n",
    "    if len(units_good) < 1:\n",
    "        continue\n",
    "\n",
    "    #Wrangle stimulus-relevant data\n",
    "    stim_df     = stim_df.loc[(stim_df.stimulus == stim)]\n",
    "    stim_times  = stim_df.loc[:,'start_time'].values\n",
    "    stim_length = stim_times[-1]-stim_times[0]\n",
    "    \n",
    "    #Organize spike data (y)\n",
    "    n_trials = len(stim_times)\n",
    "    n_units  = len(units_good)\n",
    "    \n",
    "    yall = np.zeros((n_trials,n_bins,n_units))\n",
    "\n",
    "    probes = []\n",
    "    cIDs   = []\n",
    "    rec_data = []\n",
    "    k = 0\n",
    "    for i,row in units_good.iterrows():\n",
    "        spike_times = np.array(row.times)\n",
    "        stim_spikes = spike_times[(spike_times > stim_times[0]) & (spike_times < stim_times[-1]+0.5)]\n",
    "\n",
    "        if len(stim_spikes)/stim_length < 0.5:\n",
    "            continue\n",
    "        \n",
    "        psth,var,edges,bytrial = trial_by_trial(stim_spikes, stim_times, 0.100, 0.400, bin_)\n",
    "        \n",
    "        rec_data.append(bytrial.ravel())\n",
    "        probes.append(row.probe)\n",
    "        cIDs.append(row.cluster_id)\n",
    "        k +=1\n",
    "\n",
    "    if k == 0:\n",
    "        print('No units passed firing-rate threshold')\n",
    "        continue\n",
    "    \n",
    "    #Organize stimulus data\n",
    "    sequence      = np.zeros(len(stim_times))\n",
    "    sequence[::2] = 1\n",
    "\n",
    "    conditions = [sequence]\n",
    "\n",
    "    Xall = np.ones((n_trials,n_bins,len(conditions)+1))\n",
    "\n",
    "    for s in range(len(stim_times)):\n",
    "        for c,cond in enumerate(conditions):\n",
    "            Xall[s,:int(0.100/bin_)+1,c]                  = 0.5\n",
    "            Xall[s,int(0.100/bin_)+1:int(0.050/bin_)+1,c] = cond[s]\n",
    "            Xall[s,int(0.050/bin_)+1:,c]                  = 0.5\n",
    "\n",
    "    # Shape: (n_trials*n_bins, n_units)\n",
    "    data_tensor = torch.from_numpy(np.array(rec_data).T)\n",
    "    datas.append(data_tensor)\n",
    "    all_probes.append(np.array(probes))\n",
    "    all_ids.append(np.array(cIDs))\n",
    "\n",
    "    print(f'{mouse}: {data_tensor.shape}')        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save datas and recordings \n",
    "to_save = {\n",
    "    'datas': datas,\n",
    "    'recordings': recordings\n",
    "}\n",
    "\n",
    "with open('C:/Users/denmanlab/Desktop/Emily_rotation/data_to_run_at_home/LGNluminance_flash.pkl', 'wb') as f:\n",
    "    pkl.dump(to_save, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
